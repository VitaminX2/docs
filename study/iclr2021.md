# Curated papers on ICLR 2021

## Papers of my interests

[Neural Delay Differential Equations](https://arxiv.org/abs/2102.10801)
> Qunxi Zhu, Yao Guo, Wei Lin  

[BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization](https://arxiv.org/abs/2102.10462)
> Huanrui Yang, Lin Duan, Yiran Chen, Hai Li  

[On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers](https://arxiv.org/abs/2102.07346)
> Kenji Kawaguchi  

[Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective](https://arxiv.org/abs/2102.00650)
> Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, Qian Zhang  

[DeLighT: Deep and Light-weight Transformer](https://arxiv.org/abs/2008.00623)
> Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi  

[Gradient Origin Networks](https://arxiv.org/abs/2007.02798)
> Sam Bond-Taylor, Chris G. Willcocks  

[Evaluating the Disentanglement of Deep Generative Models through Manifold Topology](https://arxiv.org/abs/2006.03680)
> Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar Carlsson, Stefano Ermon  

[Shapley explainability on the data manifold](https://arxiv.org/abs/2006.01272)
> Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, Ilya Feige  

[Anytime Sampling for Autoregressive Models via Ordered Autoencoding](https://arxiv.org/abs/2102.11495)
> Yilun Xu, Yang Song, Sahaj Garg, Linyuan Gong, Rui Shu, Aditya Grover, Stefano Ermon  

[Colorization Transformer](https://arxiv.org/abs/2102.04432)
> Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner  
